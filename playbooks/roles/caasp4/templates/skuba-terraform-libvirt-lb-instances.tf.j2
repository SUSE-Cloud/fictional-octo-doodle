data "template_file" "lb_repositories" {
  count    = "${length(var.lb_repositories)}"
  template = "${file("cloud-init/repository.tpl")}"

  vars {
    repository_url  = "${element(values(var.lb_repositories), count.index)}"
    repository_name = "${element(keys(var.lb_repositories), count.index)}"
  }
}

# Change from skuba repo version: Start
# Qemu-guest-agent install and SCC support for lb
data "template_file" "lb_register_scc" {
  template = "${file("cloud-init/register-scc.tpl")}"
  count    = "${var.caasp_registry_code == "" ? 0 : 1}"

  vars {
    caasp_registry_code = "${var.caasp_registry_code}"
    sles_registry_code = "${var.sles_registry_code}"
  }
}

data "template_file" "lb_commands" {
  template = "${file("cloud-init/commands.tpl")}"
  count    = "${join("", var.packages) == "" ? 0 : 1}"

  vars {
    packages = "${join(", ", var.packages)}"
  }
}
# Change from skuba repo version: End

data "template_file" "haproxy_apiserver_backends_master" {
  count    = "${var.masters}"
  template = "server $${fqdn} $${ip}:6443 check check-ssl verify none\n"

  vars = {
    fqdn = "${var.stack_name}-master-${count.index}.${var.dns_domain}"
    ip   = "${cidrhost(var.network_cidr, var.cidrhost_master_offset + count.index)}"
  }
}

data "template_file" "haproxy_gangway_backends_master" {
  count    = "${var.masters}"
  template = "server $${fqdn} $${ip}:32001 check check-ssl verify none\n"

  vars = {
    fqdn = "${var.stack_name}-master-${count.index}.${var.dns_domain}"
    ip   = "${cidrhost(var.network_cidr, var.cidrhost_master_offset + count.index)}"
  }
}

data "template_file" "haproxy_dex_backends_master" {
  count    = "${var.masters}"
  template = "server $${fqdn} $${ip}:32000 check check-ssl verify none\n"

  vars = {
    fqdn = "${var.stack_name}-master-${count.index}.${var.dns_domain}"
    ip   = "${cidrhost(var.network_cidr, var.cidrhost_master_offset + count.index)}"
  }
}

data "template_file" "lb_cloud_init_userdata" {
  count    = "${var.lbs}"
  template = "${file("cloud-init/lb.tpl")}"

  vars {
    apiserver_backends = "${join("      ", data.template_file.haproxy_apiserver_backends_master.*.rendered)}"
    gangway_backends   = "${join("      ", data.template_file.haproxy_gangway_backends_master.*.rendered)}"
    dex_backends       = "${join("      ", data.template_file.haproxy_dex_backends_master.*.rendered)}"
    authorized_keys    = "${join("\n", formatlist("  - %s", var.authorized_keys))}"
    repositories       = "${join("\n", data.template_file.lb_repositories.*.rendered)}"
# Change from skuba repo version: Start
# Need to install qemu-guest-agent for bridge network case
    register_scc       = "${join("\n", data.template_file.master_register_scc.*.rendered)}"
    commands           = "${join("\n", data.template_file.master_commands.*.rendered)}"
# Change from skuba repo version: End
    username           = "${var.username}"
    password           = "${var.password}"
    ntp_servers        = "${join("\n", formatlist ("    - %s", var.ntp_servers))}"
  }
}

resource "libvirt_volume" "lb" {
  name           = "${var.stack_name}-lb-volume"
  pool           = "${var.pool}"
  size           = "${var.disk_size}"
  base_volume_id = "${libvirt_volume.img.id}"
}

resource "libvirt_cloudinit_disk" "lb" {
  name = "${var.stack_name}-lib-cloudinit-disk"
  pool = "${var.pool}"

  user_data = "${data.template_file.lb_cloud_init_userdata.rendered}"
}

resource "libvirt_domain" "lb" {
  name      = "${var.stack_name}-lb-domain"
  memory    = "${var.lb_memory}"
  vcpu      = "${var.lb_vcpu}"
  cloudinit = "${libvirt_cloudinit_disk.lb.id}"
# Change from skuba repo version: Start
  qemu_agent = "true"
# Change from skuba repo version: End

  cpu {
    mode = "host-passthrough"
  }

  disk {
    volume_id = "${libvirt_volume.lb.id}"
  }

  network_interface {
# Change from skuba repo version: Start
# Network create won't work if running from existing VM on libvirt host
# as its different virtual network from VM. So need to use existing network
{% if terraform_libvirt_create_network %}
    network_id     = "${libvirt_network.network.id}"
{% else %}
    network_name     = "{{ terraform_libvirt_existing_network_name }}"
{% endif %}
    hostname       = "${var.stack_name}-lb"
{% if terraform_libvirt_use_static_ips %}
    addresses      = ["${cidrhost(var.network_cidr, var.cidrhost_lb_offset)}"]
{% endif %}
# Change from skuba repo version: End
    wait_for_lease = 1
  }

  graphics {
    type        = "vnc"
    listen_type = "address"
  }

# Change from skuba repo version: Start
  timeouts {
    create = "${var.domain_create_timeout}"
  }
# Change from skuba repo version: End
}

resource "null_resource" "lb_wait_cloudinit" {
  depends_on = ["libvirt_domain.lb"]
  count      = "${var.lbs}"

  connection {
    host     = "${element(libvirt_domain.lb.*.network_interface.0.addresses.0, count.index)}"
    user     = "${var.username}"
    password = "${var.password}"
    type     = "ssh"
  }

  provisioner "remote-exec" {
    inline = [
      "cloud-init status --wait > /dev/null",
    ]
  }
}

resource "null_resource" "lb_reboot" {
  depends_on = ["null_resource.lb_wait_cloudinit"]
  count      = "${var.lbs}"

  provisioner "local-exec" {
    environment = {
      user = "${var.username}"
      host = "${element(libvirt_domain.lb.*.network_interface.0.addresses.0, count.index)}"
    }

    command = <<EOT
ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null $user@$host sudo reboot || :
# wait for ssh ready after reboot
ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -oConnectionAttempts=60 $user@$host /usr/bin/true
EOT
  }
}